[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Gumbel Softmax: Differentiable Sampling from Discrete Distributions\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nDec 23, 2024\n\n\nAbhyuday\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 26, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 23, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/differentiable-sampling/index.html",
    "href": "posts/differentiable-sampling/index.html",
    "title": "Gumbel Softmax: Differentiable Sampling from Discrete Distributions",
    "section": "",
    "text": "Discrete sampling plays a crucial role in many machine learning tasks, such as classification, reinforcement learning, and generative modeling. Whether you’re predicting the next action for a reinforcement learning agent or selecting vectors from a codebook in a VQ-VAE, the process often involves choosing one value from a predefined set of categories based on a probability distribution.\nHowever, during training, this sampling process encounters a major roadblock: the lack of differentiability. This makes backpropagation—essential for optimizing neural networks—impossible. Enter Gumbel-Softmax, a technique that enables differentiable sampling while retaining the key properties of discrete sampling. In this blog post, we’ll explore the problem, dissect the Gumbel-Softmax trick, and understand how it solves this challenge.\n\nWhat is Discrete Sampling\nDiscrete sampling refers to selecting the value of a random variable from a given discrete probability distribution \\(p\\), defined as:\n\\[\np = [p_1, p_2, ..., p_n] \\quad where \\quad \\sum_{i=1}^n p_i = 1\n\\]\nFor example, \\(p = [0.6, 0.3, 0.1]\\) is a categorical distribution over three classes,\n\nWe can select first class with probability: \\(P(X=1) = 0.6\\) i.e. 60%\nWe can select second class with probability: \\(P(X=2) = 0.3\\) i.e. 30%\nWe can select third class with probability: \\(P(X=3) = 0.1\\) i.e. 10%\n\nIn practice, Sampling process is usually performed using:\n\nargmax selecting the value with highest probability. e.g. inference on a multi-class classifier.\nnp.random.choice selecting a random item with corresponding probability. e.g. a RL agent exploring the discrete action space.\n\n\n\n\nProblem with Discrete Sampling\nSampling methods mentioned in the previous section provide options for both exploration (during data collection) and exploitation (during inference) of the distribution space.\nBut, what if sampling is required during training phase e.g sampling vectors from a codebook while training a VQVAE or in tasks like reinforcement learning or generative modeling, where sampling is necessary to explore different actions or outputs.\nWell, the training stage requires all the intermediary operations to be differentiable, for the mighty Backpropagation. Let’s analyze the differentiability of sampling options available to us:\n\nargmax being a discontinuous function everywhere, i.e. \\(\\quad\\lim_{h\\to0}f(x+h) \\neq \\lim_{h\\to0}f(x+h) \\neq f(x)\\) is clearly not differentiable.\nrandom selection methods like np.random.choice are also non-differentiable.\n\nNow that we have a clear picture of the problem, lets build up to the Gumbel Softmax and find out how it overcomes the impediment of non-differentiability.\n\n\nGumbel Softmax\nAs mentioned in the previous section, we need a differentiable solution for argmax-styled determinism and stochastic nature of random selection to maintain the exploration & exploitation.\nWe’ll now discuss the proposed two-fold solution:\n\nSoftmax with temperature\nThe softmax with temperature modifies the output probabilities of a categorical distribution by scaling the logits \\(z\\) with a temperature parameter \\(\\tau\\):\n\\[\n\\text{softmax}(z_i, \\tau) = \\frac{e^{z_i/\\tau}}{\\sum_{j=1}^{n} e^{z_j/\\tau}}\n\\]\n\nHigh Temperature (\\(\\tau \\to \\infty\\)): Makes probabilities more uniform\nLow Temperature (\\(\\tau \\to 0\\)): Sharpens the probability, approximation of one-hot vector\n\n\nfunction softmax(logits, temp){\n  let sum=0\n  for (let index = 0; index &lt; logits.length; index++) {\n    sum+= Math.exp((logits[index] + 1e-8)/temp)\n  }\n  let probs = logits.map(x=&gt; Math.exp((x + 1e-8)/temp)/sum)\n  return probs\n}\nprob = Array.from({length: 5}, ()=&gt;Math.random()*10)\nviewof gain = Inputs.range([0.1, 50], {value: 20, step: 0.1, label: \"Temperature (τ)   \"})\nPlot.barY(softmax(prob, gain)\n ).plot({x: {label: \"Categories\"}, y: {label: \"Probs\"}})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis approach provides a differentiable alternative to argmax-styled one-hot approximation (at low temperatures). However, it still fails to introduce stochasticity at low temperatures. It always outputs a deterministic distribution for a given set of logits \\(z\\) .\n\n\nGumbel Noise\nStochasticity is needed to encourage exploration and mimic random sampling from given categorical distribution. We can inject this stochasticity through perturbing the logits by adding a noise \\(\\epsilon\\) value before applying softmax.\nGumbel noise emerges as the perfect candidate for this because of following properties:\n\nIt is mathematically aligned with categorical distributions and enables differentiable approximation of discrete sampling via the Gumbel-Softmax trick.\nIt introduces randomness in a way that respects the structure of categorical distributions while enabling gradient-based optimization.\n\nGumble noise \\(\\epsilon\\) is sampled from the standard Gumbel distribution. We can draw such noise samples by first computing \\(u \\sim Uniform(0, 1)\\) and then computing \\(\\epsilon = -\\log(-\\log(u))\\) .\nNow putting it all together we have the Gumble Softmax Equation as:\n\\[\nGumbelSoftmax(z_i, \\tau) =  \\frac{e^{(z_i + \\epsilon)/\\tau}}{\\sum_{j=1}^{n} e^{(z_j + \\epsilon)/\\tau}}\n\\]\n\nHow it Works:\n\nStochasticity: \\(\\epsilon\\) injects randomness, enabling exploration.\nDifferentiability: The softmax function ensures the operation is differentiable.\nControl: \\(\\tau\\) adjusts the trade-off between exploration and exploitation:"
  }
]