---
title: "Gumble Softmax: Differentiable Sampling from Discrete Distributions"
date: "2024-12-23"
author: "Abhyuday"
execute:
  echo: false
  enabled: true
format: 
  html:
    widgets: true

engine: jupyter
categories: [code]
---

# What is Discrete Sampling

Discrete sampling refers to the process of selecting value of a random variable over a given discrete probability distribution *p*.

$$
p = [p_1, p_2, ..., p_n] \quad where \quad \sum_{i=1}^n p_i = 1
$$

For example, $p = [0.6, 0.3, 0.1]$ is a categorical distribution over three classes,

-   We can select first class with probability: $P(X=1) = 0.6$ \quad i.e. 60%

-   We can select second class with probability: $P(X=2) = 0.3$ \quad i.e. 30%

-   We can select third class with probability: $P(X=3) = 0.1$ \quad i.e. 10%

In practice, Sampling process is usually performed using:

1.  `argmax` selecting the value with highest probability. e.g. inference on a multi-class classifier.

2.  `np.random.choice` selecting a random item with corresponding probability. e.g. a RL agent exploring the discrete action space.

# Problem with Discrete Sampling

Sampling methods mentioned in the previous section provide options for both exploration (during data collection) and exploitation (during inference) of the distribution space.

But, what if sampling is required during training phase e.g sampling vectors from a codebook while training a VQVAE or in tasks like reinforcement learning or generative modeling, where sampling is necessary to explore different actions or outputs.

Well, the training stage requires all the intermediary operations to be differentiable, for the mighty *Backpropagation*. Let's analyze the differentiability of sampling options available to us:

1.  `argmax` being a discontinuous function everywhere, i.e. $\quad\lim_{h\to0}f(x+h) \neq \lim_{h\to0}f(x+h) \neq f(x)$ is clearly not differentiable.
2.  random selection methods like `np.random.choice` are also non-differentiable.

Now that we have a clear picture of the problem, lets build up to the Gumble Softmax and find out how it overcomes the impediment of non-differentiability.

# Gumble Softmax

Before jumping directly into the mathematical equations and derivations

```{python}
import numpy as np
import bqplot as bq
import ipywidgets as widgets
from IPython.display import display

# Define data
x = np.linspace(0, 10, 100)
y = np.sin(x)

# Create a scale and axes
x_scale = bq.LinearScale()
y_scale = bq.LinearScale()

x_axis = bq.Axis(scale=x_scale, label='X-axis')
y_axis = bq.Axis(scale=y_scale, orientation='vertical', label='Y-axis')

# Create a line chart
line = bq.Lines(x=x, y=y, scales={'x': x_scale, 'y': y_scale})

# Create a figure
fig = bq.Figure(marks=[line], axes=[x_axis, y_axis], title="Sine Wave with Amplitude Control")

# Create a slider widget
slider = widgets.FloatSlider(
    value=1.0,
    min=0.1,
    max=2.0,
    step=0.1,
    description='Amplitude:',
)

# Define the slider's callback
def update_wave(change):
    amplitude = slider.value
    line.y = amplitude * np.sin(x)

# Link the slider to the plot
slider.observe(update_wave, names='value')

# Display the slider and plot
display(slider, fig)
```